{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classifer_corpus.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1hwtVwJ2sKw53LpOBTpLsJOaXn3EfDejJ",
      "authorship_tag": "ABX9TyNgIP7EZilPkB8ALMXtqzY0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alt41r/ML/blob/master/classifer_corpus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoRvnAhOvRR1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9592cfc-d6d0-47cc-de24-fb51ca18c328"
      },
      "source": [
        "!pip3 install readability-lxml\n",
        "import unicodedata\n",
        "import nltk.corpus\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "from collections import defaultdict\n",
        "import codecs\n",
        "import bs4\n",
        "from nltk import sent_tokenize\n",
        "from nltk import wordpunct_tokenize\n",
        "from nltk import pos_tag\n",
        "import time\n",
        "from nltk.corpus.reader.api import CorpusReader\n",
        "from nltk.corpus.reader.api import CategorizedCorpusReader\n",
        "from readability.readability import Unparseable\n",
        "from readability.readability import Document as Paper\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score,precision_score, recall_score\n",
        "import nltk\n",
        "from sklearn.metrics import classification_report\n",
        "import tabulate\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, language='english'):\n",
        "        self.stopwords  = set(nltk.corpus.stopwords.words(language))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def is_punct(self, token):\n",
        "        return all(\n",
        "            unicodedata.category(char).startswith('P') for char in token\n",
        "        )\n",
        "\n",
        "    def is_stopword(self, token):\n",
        "        return token.lower() in self.stopwords\n",
        "\n",
        "    def normalize(self, document):\n",
        "        return [\n",
        "            self.lemmatize(token, tag).lower()\n",
        "            for paragraph in document\n",
        "            for sentence in paragraph\n",
        "            for (token, tag) in sentence\n",
        "            if not self.is_punct(token) and not self.is_stopword(token)\n",
        "        ]\n",
        "\n",
        "    def lemmatize(self, token, pos_tag):\n",
        "        tag = {\n",
        "            'N': wn.NOUN,\n",
        "            'V': wn.VERB,\n",
        "            'R': wn.ADV,\n",
        "            'J': wn.ADJ\n",
        "        }.get(pos_tag[0], wn.NOUN)\n",
        "\n",
        "        return self.lemmatizer.lemmatize(token, tag)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, documents):\n",
        "        for document in documents:\n",
        "            yield self.normalize(document[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: readability-lxml in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: cssselect in /usr/local/lib/python3.7/dist-packages (from readability-lxml) (1.1.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from readability-lxml) (3.0.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from readability-lxml) (4.2.6)\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALM4me4FvsQt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqShWuMZvnzX"
      },
      "source": [
        "\n",
        "\n",
        "CAT_PATTERN = r'([a-z_\\s]+)/.*'\n",
        "DOC_PATTERN = r'(?!\\.)[a-z_\\s]+/[a-f0-9]+\\.json'\n",
        "TAGS = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'p', 'li']\n",
        "tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'p', 'li']\n",
        "\n",
        "\n",
        "class HTMLCorpusReader(CategorizedCorpusReader, CorpusReader):\n",
        "    \"\"\"    Объект чтения корпуса сHTML-документами для получения\n",
        "         возможности дополнительной предварительной обработки.    \"\"\"\n",
        "\n",
        "    def __init__(self, root, fileids=DOC_PATTERN, encoding='utf8', tags=TAGS, **kwargs):\n",
        "        \"\"\"Инициализирует объект чтения корпуса.Аргументы, управляющие классификацией (``cat_pattern``, ``cat_map``\n",
        "        и``cat_file``), передаются в конструктор ``CategorizedCorpusReader``. остальные аргументы передаются\n",
        "        вконструктор ``CorpusReader``.\"\"\"\n",
        "        # Добавить шаблон категорий, если он не был передан в класс явно\n",
        "        if not any(key.startswith('cat_') for key in kwargs.keys()):\n",
        "            kwargs['cat_pattern'] = CAT_PATTERN\n",
        "\n",
        "            # Инициализировать объекты чтения корпуса из NLTK\n",
        "            CategorizedCorpusReader.__init__(self, kwargs)\n",
        "            CorpusReader.__init__(self, root, fileids, encoding)\n",
        "\n",
        "            # Сохранить теги, подлежащие извлечению\n",
        "            self.tags = tags\n",
        "\n",
        "    def resolve(self, fileids, categories):\n",
        "        \"\"\"Возвращает список идентификаторов файлов или названий категорий,которые передаются каждой\n",
        "        внутренней функции объекта чтения корпуса. Реализована по аналогии с``CategorizedPlaintextCorpusReader``\n",
        "        в NLTK. \"\"\"\n",
        "        if fileids is not None and categories is not None:\n",
        "            raise ValueError(\"Specify fileids or categories, not both\")\n",
        "        if categories is not None:\n",
        "            return self.fileids(categories)\n",
        "        return fileids\n",
        "\n",
        "    def docs(self, fileids=None, categories=None):\n",
        "        \"\"\"        Возвращает полный текст HTML-документа, закрывая его         по завершении чтения.        \"\"\"\n",
        "        # Получить список файлов для чтени\n",
        "        fileids = self.resolve(fileids, categories)\n",
        "\n",
        "        # Cоздать    генератор, загружающий    документы    в память    по    одному\n",
        "        for path, encoding in self.abspaths(fileids, include_encoding=True):\n",
        "            with codecs.open(path, 'r', encoding=encoding) as f:\n",
        "                yield f.read()\n",
        "\n",
        "    def sizes(self, fileids=None, categories=None):\n",
        "        \"\"\"Возвращает список кортежей, идентификатор файла иего размер.Эта функция используется для выявления\n",
        "        необычно больших файлов в корпусе. \"\"\"\n",
        "\n",
        "        # Получить список файлов\n",
        "        fileids = self.resolve(fileids, categories)\n",
        "\n",
        "        # Создать генератор, возвращающий имена иразмеры файлов\n",
        "        for path in self.abspaths(fileids):\n",
        "            yield path, os.path.getsize(path)\n",
        "\n",
        "    def html(self, fileids=None, categories=None):\n",
        "        \"\"\"Возвращает содержимое HTML каждого документа, очищая его\n",
        "        с помощью библиотеки readability-lxml.\"\"\"\n",
        "\n",
        "        for doc in self.docs(fileids, categories):\n",
        "            try:\n",
        "                yield Paper(doc).summary()\n",
        "            except Unparseable as e:\n",
        "                print(\"Could not parse HTML: {}\".format(e))\n",
        "            continue\n",
        "\n",
        "        log = logging.getLogger(\"readability.readability\")\n",
        "        log.setLevel('WARNING')\n",
        "\n",
        "    def paras(self, fileids=None, categories=None):\n",
        "        for html in self.html(fileids, categories):\n",
        "            soup = bs4.BeautifulSoup(html, 'lxml')\n",
        "            for element in soup.find_all(tags):\n",
        "                yield element.text\n",
        "            soup.decompose()\n",
        "\n",
        "    def sents(self, fileids=None, categories=None):\n",
        "        \"\"\"Использует встроенный механизм для выделения предложений из\n",
        "        абзацев. Обратите внимание, что для парсинга разметки HTML\n",
        "        этот метод использует BeautifulSoup.\"\"\"\n",
        "        for paragraph in self.paras(fileids, categories):\n",
        "            for sentence in sent_tokenize(paragraph):\n",
        "                yield sentence\n",
        "\n",
        "    def words(self, fileids=None, categories=None):\n",
        "        \"\"\"Использует встроенный механизм для выделения слов из предложений.\n",
        "        Обратите внимание, что для парсинга разметки HTML\n",
        "        этот метод использует BeautifulSoup\"\"\"\n",
        "        for sentence in self.sents(fileids, categories):\n",
        "            for token in wordpunct_tokenize(sentence):\n",
        "                yield token\n",
        "\n",
        "    def tokenize(self, fileids=None, categories=None):\n",
        "        \"\"\"Сегментирует, лексемизирует и маркирует документ в корпусе.\"\"\"\n",
        "        for paragraph in self.paras(fileids=fileids):\n",
        "            yield [\n",
        "                pos_tag(wordpunct_tokenize(sent))\n",
        "                for sent in sent_tokenize(paragraph)\n",
        "            ]\n",
        "\n",
        "    def descibe(self, fileids=None, categories=None):\n",
        "        \"\"\"Выполняет обход содержимого корпуса ивозвращает\n",
        "        словарь сразнообразными оценками, описывающими\n",
        "        состояние корпуса.\"\"\"\n",
        "        started = time.time()\n",
        "\n",
        "        # Структуры для подсчета\n",
        "        counts = nltk.FreqDist()\n",
        "        tokens = nltk.FreqDist()\n",
        "\n",
        "        # Выполнить обход абзацев, выделить лексемы иподсчитать их\n",
        "        for para in self.paras(fileids, categories):\n",
        "            counts['para'] += 1\n",
        "\n",
        "            for sent in para:\n",
        "                counts['sents'] += 1\n",
        "                for word, tag in sent:\n",
        "                    counts['words'] += 1\n",
        "                    tokens[word] += 1\n",
        "\n",
        "        # Определить число файлов и категорий в корпусе\n",
        "        n_fileids = len(self.resolve(fileids, categories) or self.fileids())\n",
        "        n_topics = len(self.categories(self.resolve(fileids, categories)))\n",
        "\n",
        "        # Вернуть структуру данных с информацией\n",
        "        return {'files': n_fileids,\n",
        "                'topics': n_topics,\n",
        "                'paras': counts['paras'],\n",
        "                'sents': counts['sents'],\n",
        "                'words': counts['words'],\n",
        "                'vocab': len(tokens),\n",
        "                'lexdiv': float(counts['words']) / float(len(tokens)),\n",
        "                'ppdoc': float(counts['paras']) / float(n_fileids),\n",
        "                'sppar': float(counts['sents']) / float(counts['paras']),\n",
        "                'secs': time.time() - started, }\n",
        "\n",
        "\n",
        "class Preprocessor(object):\n",
        "    \"\"\"Обертывает `HTMLCorpusReader` ивыполняет лексемизацию\n",
        "        смаркировкой частями речи.\"\"\"\n",
        "\n",
        "    def __init__(self, corpus, target=None, **kwargs):\n",
        "        self.corpus = corpus\n",
        "        self.target = target\n",
        "\n",
        "    def fileids(self, fileids=None, categories=None):\n",
        "        fileids = self.corpus.resolve(fileids, categories)\n",
        "        if fileids:\n",
        "            return fileids\n",
        "        return self.corpus.fileids()\n",
        "\n",
        "    def abspath(self, fileid):\n",
        "        # Найти путь к каталогу относительно корня исходного корпуса.\n",
        "        parent = os.path.relpath(\n",
        "            os.path.dirname(self.corpus.abspath(fileid)), self.corpus.root\n",
        "        )\n",
        "        # Выделить части пути для реконструирования\n",
        "        basename = os.path.basename(fileid)\n",
        "        name, ext = os.path.splitext(basename)\n",
        "\n",
        "        # Сконструировать имя файла срасширением .pickle\n",
        "        basename = name + '.pickle'\n",
        "\n",
        "        # Вернуть путь кфайлу относительно корня целевого корпуса.\n",
        "        return os.path.normpath(os.path.join(self.target, parent, basename))\n",
        "\n",
        "    def tokenize(self, fileids=None, categories=None):\n",
        "        \"\"\"Сегментирует, лексемизирует и маркирует документ в корпусе.\"\"\"\n",
        "        for paragraph in self.paras(fileids=fileids):\n",
        "            yield [\n",
        "                pos_tag(wordpunct_tokenize(sent))\n",
        "                for sent in sent_tokenize(paragraph)\n",
        "            ]\n",
        "\n",
        "    def process(self, fileid):\n",
        "        \"\"\"Вызывается для одного файла, проверяет местоположение на диске,\n",
        "            чтобы гарантировать отсутствие ошибок, использует +tokenize()+ для\n",
        "            предварительной обработки и записывает трансформированный документ\n",
        "            в виде сжатого архива в заданное место.\"\"\"\n",
        "\n",
        "        # Определить путь кфайлу для записи результата.\n",
        "        target = self.abspath(fileid)\n",
        "        parent = os.path.dirname(target)\n",
        "\n",
        "        # Убедиться всуществовании каталога\n",
        "        if not os.path.exists(parent):\n",
        "            os.makedirs(parent)\n",
        "\n",
        "        # Убедиться, что parent— это каталог, а не файл\n",
        "        if not os.path.isdir(parent):\n",
        "            raise ValueError(\n",
        "                'Please supply a directory to write preprocessed data to. '\n",
        "            )\n",
        "        # Создать структуру данных для записи вархив\n",
        "        document = list(self.tokenize(fileid))\n",
        "\n",
        "        # Записать данные вархив на диск\n",
        "        with open(target, 'wb') as f:\n",
        "            pickle.dump(document, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "        # Удалить документ из памяти\n",
        "        del document\n",
        "\n",
        "        # Вернуть путь кцелевому файлу\n",
        "        return target\n",
        "\n",
        "    def transform(self, fileids=None, categories=None):\n",
        "        # Создать целевой каталог, если его еще нет\n",
        "        if not os.path.exists(self.target):\n",
        "            os.makedirs(self.target)\n",
        "\n",
        "        # Получить имена файлов для обработки\n",
        "        for fileid in self.fileids(fileids, categories):\n",
        "            yield self.process(fileid)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc-kyA8ZvaQB"
      },
      "source": [
        "\n",
        "\n",
        "PKL_PATTERN = r'(?!\\.)[a-z_\\s]+/[a-f0-9]+\\.pickle'\n",
        "CAT_PATTERN = r'([a-z_\\s]+)/.*'\n",
        "\n",
        "class PickledCorpusReader(HTMLCorpusReader):\n",
        "    def __init__(self, root, fileids=PKL_PATTERN, **kwargs):\n",
        "        if not any(key.startswith('cat_') for key in kwargs.keys()):\n",
        "            kwargs['cat_pattern'] = CAT_PATTERN\n",
        "        CategorizedCorpusReader.__init__(self, kwargs)\n",
        "        CorpusReader.__init__(self, root, fileids)\n",
        "\n",
        "    def docs(self, fileids=None, categories=None):\n",
        "        fileids = self.resolve(fileids, categories)\n",
        "\n",
        "        # Загружать документы впамять по одному.\n",
        "        for path in self.abspaths(fileids):\n",
        "            with open(path, 'rb') as f:\n",
        "                yield pickle.load(f)\n",
        "\n",
        "    def paras(self, fileids=None, categories=None):\n",
        "        for doc in self.docs(fileids, categories):\n",
        "            for para in doc:\n",
        "                yield para\n",
        "\n",
        "    def sents(self, fileids=None, categories=None):\n",
        "        for para in self.paras(fileids, categories):\n",
        "            for sent in para:\n",
        "                yield sent\n",
        "\n",
        "    def tagged(self,fileids=None,categories=None):\n",
        "        for sent in self.sents(fileids,categories):\n",
        "            for tagged_token in sent:\n",
        "                yield tagged_token\n",
        "\n",
        "    def words(self, fileids=None, categories=None):\n",
        "        for tagged in self.tagged(fileids,categories):\n",
        "            yield tagged[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyA3gYZov-Zs"
      },
      "source": [
        "\n",
        "class CorpusLoader(object):\n",
        "\n",
        "    def __init__(self, reader, folds=12, shuffle=True, categories=None):\n",
        "        self.reader = reader\n",
        "        self.folds  = KFold(n_splits=folds, shuffle=shuffle)\n",
        "        self.files  = np.asarray(self.reader.fileids(categories=categories))\n",
        "\n",
        "    def fileids(self, idx=None):\n",
        "        if idx is None:\n",
        "            return self.files\n",
        "        return self.files[idx]\n",
        "\n",
        "    def documents(self, idx=None):\n",
        "        for fileid in self.fileids(idx):\n",
        "            yield list(self.reader.docs(fileids=[fileid]))\n",
        "\n",
        "    def labels(self, idx=None):\n",
        "        return [\n",
        "            self.reader.categories(fileids=[fileid])[0]\n",
        "            for fileid in self.fileids(idx)\n",
        "        ]\n",
        "\n",
        "    def __iter__(self):\n",
        "        for train_index, test_index in self.folds.split(self.files):\n",
        "            X_train = self.documents(train_index)\n",
        "            y_train = self.labels(train_index)\n",
        "\n",
        "            X_test = self.documents(test_index)\n",
        "            y_test = self.labels(test_index)\n",
        "\n",
        "            yield X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "\n",
        "def identity(words):\n",
        "    return words\n",
        "\n",
        "\n",
        "def create_pipeline(estimator, reduction=False):\n",
        "    steps = [\n",
        "        ('normalize', TextNormalizer()),\n",
        "        ('vectorize', TfidfVectorizer(\n",
        "            tokenizer=identity, preprocessor=None, lowercase=False))\n",
        "    ]\n",
        "    if reduction:\n",
        "        steps.append((\n",
        "            'reduction', TruncatedSVD(n_components=10000)\n",
        "\n",
        "        ))\n",
        "    # добавить обьект оценки\n",
        "    steps.append(('classifier', estimator))\n",
        "    return Pipeline(steps)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrxIN5ywTolf"
      },
      "source": [
        "import json\n",
        "\n",
        "models = []\n",
        "for form in (LogisticRegression, SGDClassifier):\n",
        "    models.append(create_pipeline(form(), True))\n",
        "    models.append(create_pipeline(form(), False))\n",
        "\n",
        "models.append(create_pipeline(MultinomialNB(), False))\n",
        "models.append(create_pipeline(GaussianNB(), True))\n",
        "\n",
        "reader = PickledCorpusReader('/content/drive/MyDrive/sample')\n",
        "labels = ['books', 'cinema', 'cooking', 'gaming', 'sports', 'tech']\n",
        "loader = CorpusLoader(reader, 5, shuffle=True, categories=labels)\n",
        "fields = ['model','precision','recall','accuracy','f1']\n",
        "table = []\n",
        "for model in models:\n",
        "\n",
        "        name = model.named_steps['classifier'].__class__.__name__\n",
        "        if 'reduction' in model.named_steps:\n",
        "            name += \" (TruncatedSVD)\"\n",
        "\n",
        "        scores = {\n",
        "            'model': str(model),\n",
        "            'name': name,\n",
        "            'accuracy': [],\n",
        "            'precision': [],\n",
        "            'recall': [],\n",
        "            'f1': [],\n",
        "            'time': [],\n",
        "        }\n",
        "\n",
        "        for X_train, X_test, y_train, y_test in loader:\n",
        "            start = time.time()\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            scores['time'].append(time.time() - start)\n",
        "            scores['accuracy'].append(accuracy_score(y_test, y_pred))\n",
        "            scores['precision'].append(precision_score(y_test, y_pred, average='weighted'))\n",
        "            scores['recall'].append(recall_score(y_test, y_pred, average='weighted'))\n",
        "            scores['f1'].append(f1_score(y_test, y_pred, average='weighted'))\n",
        "            row = [str(model)]\n",
        "            for field in fields[1:]:\n",
        "                row.append(np.mean(scores[field]))\n",
        "\n",
        "            table.append(row)\n",
        "table.sort(key=lambda row: row[-1], reverse=True)\n",
        "print(tabulate.tabulate(table, headers=fields))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_t2RMOJwVko"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeIpGBNTwYp6"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}